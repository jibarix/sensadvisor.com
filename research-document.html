<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TECHNICAL REPORT: Core Architectures of Text Diffusion Models</title>
    
    <!-- Google Fonts - Source Code Pro -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Source Code Pro', monospace;
            background: #ffffff;
            color: #000000;
            line-height: 1.6;
            font-size: 14px;
        }

        .document-container {
            max-width: 800px;
            margin: 0 auto;
            background: #ffffff;
            min-height: 100vh;
            border-left: 2px solid #000000;
            border-right: 2px solid #000000;
        }

        /* Document Header */
        .document-header {
            background: #000000;
            color: #ffffff;
            padding: 20px;
            text-align: center;
            border-bottom: 4px solid #000000;
        }

        .classification {
            font-size: 12px;
            font-weight: 300;
            letter-spacing: 2px;
            margin-bottom: 10px;
        }

        .document-title {
            font-size: 18px;
            font-weight: 700;
            margin-bottom: 5px;
            letter-spacing: 1px;
        }

        .document-subtitle {
            font-size: 14px;
            font-weight: 400;
            margin-bottom: 15px;
        }

        .document-meta {
            font-size: 11px;
            border-top: 1px solid #ffffff;
            padding-top: 10px;
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
        }

        .meta-item {
            text-align: center;
        }

        /* Navigation */
        .nav-header {
            background: #ffffff;
            border-bottom: 2px solid #000000;
            padding: 15px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-back {
            color: #000000;
            text-decoration: none;
            font-weight: 600;
            border: 2px solid #000000;
            padding: 8px 16px;
            transition: all 0.2s ease;
        }

        .nav-back:hover {
            background: #000000;
            color: #ffffff;
        }

        .doc-info {
            font-size: 12px;
            font-weight: 400;
        }

        /* Content Area */
        .document-content {
            padding: 30px;
        }

        .section-divider {
            border: none;
            border-top: 2px solid #000000;
            margin: 30px 0;
        }

        .section-divider.heavy {
            border-top: 4px solid #000000;
            margin: 40px 0;
        }

        /* Typography */
        h1 {
            font-size: 20px;
            font-weight: 700;
            margin-bottom: 20px;
            border-bottom: 2px solid #000000;
            padding-bottom: 10px;
            letter-spacing: 1px;
            text-transform: uppercase;
        }

        h2 {
            font-size: 16px;
            font-weight: 600;
            margin: 30px 0 15px 0;
            border-left: 4px solid #000000;
            padding-left: 15px;
        }

        h3 {
            font-size: 14px;
            font-weight: 600;
            margin: 25px 0 10px 0;
            text-decoration: underline;
        }

        h4 {
            font-size: 13px;
            font-weight: 600;
            margin: 20px 0 8px 0;
            font-style: italic;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        /* Lists */
        ul, ol {
            margin: 15px 0 15px 30px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Special Elements */
        .emphasis {
            font-weight: 700;
            background: #000000;
            color: #ffffff;
            padding: 2px 4px;
        }

        .reference {
            font-style: italic;
            color: #000000;
        }

        .code-block {
            background: #f0f0f0;
            border: 2px solid #000000;
            padding: 15px;
            font-family: 'Source Code Pro', monospace;
            margin: 15px 0;
            overflow-x: auto;
        }

        /* Special Sections */
        .abstract {
            border: 2px solid #000000;
            padding: 20px;
            margin: 20px 0;
            background: #ffffff;
        }

        .abstract h2 {
            margin-top: 0;
            text-align: center;
            border: none;
            padding: 0;
        }

        /* References Section */
        .references {
            border-top: 4px solid #000000;
            padding-top: 20px;
            margin-top: 40px;
        }

        .reference-item {
            margin-bottom: 15px;
            padding-left: 20px;
            text-indent: -20px;
            font-size: 12px;
        }

        /* Footer */
        .document-footer {
            background: #000000;
            color: #ffffff;
            padding: 20px;
            text-align: center;
            margin-top: 40px;
            font-size: 12px;
        }

        .footer-grid {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin-bottom: 10px;
        }

        /* Print Styles */
        @media print {
            .nav-header {
                display: none;
            }
            
            .document-container {
                border: none;
                box-shadow: none;
            }
            
            body {
                font-size: 12px;
            }
        }

        /* Mobile Responsiveness */
        @media (max-width: 768px) {
            .document-container {
                border-left: none;
                border-right: none;
            }
            
            .document-content {
                padding: 20px;
            }
            
            .document-meta {
                grid-template-columns: 1fr;
                gap: 10px;
            }
            
            .footer-grid {
                grid-template-columns: 1fr;
                gap: 10px;
            }
            
            .nav-header {
                flex-direction: column;
                gap: 10px;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="document-container">
        <!-- Document Header -->
        <header class="document-header">
            <div class="classification">TECHNICAL RESEARCH DOCUMENT</div>
            <h1 class="document-title">CORE ARCHITECTURES OF TEXT DIFFUSION MODELS</h1>
            <div class="document-subtitle">A Developer's Guide</div>
            <div class="document-meta">
                <div class="meta-item">
                    <strong>DOCUMENT ID:</strong><br>
                    TDM_DEV_GUIDE_v0.2
                </div>
                <div class="meta-item">
                    <strong>AUTHOR:</strong><br>
                    Jorge A. Arroyo
                </div>
                <div class="meta-item">
                    <strong>STATUS:</strong><br>
                    RELEASED
                </div>
            </div>
        </header>

        <!-- Navigation -->
        <nav class="nav-header">
            <a href="index.html" class="nav-back">← RETURN TO MAIN SITE</a>
            <div class="doc-info">SENS ADVISOR TECHNICAL RESEARCH</div>
        </nav>

        <!-- Document Content -->
        <main class="document-content">
            <!-- Abstract -->
            <section class="abstract">
                <h2>EXECUTIVE SUMMARY</h2>
                <p>An analysis of recent research reveals two dominant architectural paradigms for text generation diffusion models: <span class="emphasis">Discrete Diffusion</span> and <span class="emphasis">Continuous Diffusion</span>. While both are built on the foundational principle of reversing a corruption process, their internal mechanics differ significantly. This guide outlines a core five-component architecture, providing a clear and consistent understanding of both approaches for developers.</p>
                
                <p>The central idea is a two-stage process: A <span class="emphasis">fixed, non-learned Forward Process</span> systematically corrupts clean text into a simple, known distribution, and a <span class="emphasis">learned Reverse Process</span> starts from that simple distribution and iteratively refines it back into coherent, clean text.</p>
            </section>

            <hr class="section-divider heavy">

            <!-- Main Content -->
            <h1>Current Architectural Limitations: AR vs. NAR</h1>
            
            <p>Before diving into the components, it's crucial to understand the high-level limitations inherent to both autoregressive (AR) and non-autoregressive (NAR) models like diffusion.</p>

            <h2>Autoregressive (AR) Models</h2>
            
            <p>The primary limitation of AR models stems from their sequential, left-to-right generation process.</p>

            <ul>
                <li><span class="emphasis">Error Propagation & Exposure Bias:</span> AR models are prone to a phenomenon called "exposure bias," where a discrepancy exists between training (where the model is fed ground-truth tokens) and inference (where it must rely on its own, possibly flawed, predictions). An early mistake can't be corrected and often leads to a cascade of errors, degrading the quality of the entire sequence <span class="reference">(Tang et al., 2023)</span>.</li>
                
                <li><span class="emphasis">Poor Long-Range Planning:</span> The token-by-token approach is inherently short-sighted. This makes AR models struggle with tasks requiring complex reasoning, planning, or global coherence, such as solving logic puzzles or mathematical problems. <span class="emphasis">Ye et al. (2025)</span> demonstrate that AR models fail to balance competing subgoals, leading to what they term <span class="emphasis">"The Regretful Compromise"</span>—where the model, realizing late that it cannot reach the goal, forces an incorrect final step just to satisfy the immediate constraint.</li>
                
                <li><span class="emphasis">Slow Inference:</span> Generating a sequence of length <em>N</em> requires <em>N</em> sequential forward passes through the model, making inference computationally slow, especially for long texts.</li>
            </ul>

            <h2>Non-Autoregressive (NAR) Models</h2>
            
            <p>NAR models generate all tokens in parallel, which addresses some AR limitations but introduces new challenges. While diffusion models are the most prominent example, the NAR category also includes other architectures like <span class="emphasis">Energy-Based Transformers (EBTs)</span>, which frame generation as an optimization problem rather than a denoising one <span class="reference">(Gladstone et al., 2025)</span>.</p>

            <ul>
                <li><span class="emphasis">Training-Inference Discrepancy:</span> A significant gap exists between the training and inference phases. During training, the model learns to denoise a sequence that was corrupted from clean data. During inference, however, it must denoise its own, potentially imperfect, previous outputs without the guidance of the forward corruption process <span class="reference">(Tang et al., 2023)</span>.</li>
                
                <li><span class="emphasis">Difficulty with Long, Unstructured Text:</span> While effective for structured or single-sentence tasks, diffusion models often struggle to generate long, coherent paragraphs. <span class="emphasis">Ochs & Habernal (2025)</span> found that, compared to AR models, text from diffusion models often had orders of magnitude higher perplexity and was frequently incoherent, especially when trained under noisy conditions like Differential Privacy.</li>
                
                <li><span class="emphasis">Slow Inference (Without Acceleration):</span> Although tokens are generated in parallel, the process is highly iterative. A single generation can require thousands of denoising steps, making the process extremely slow without advanced acceleration techniques <span class="reference">(Tang et al., 2023)</span>.</li>
            </ul>

            <hr class="section-divider heavy">

            <h1>The Core Trade-Off: Discrete vs. Continuous</h1>
            
            <p>The fundamental difference between these two architectures lies in the <span class="emphasis">representation of text</span> during the diffusion process, which creates a core trade-off between <span class="emphasis">representational fidelity</span> and <span class="emphasis">computational tractability</span>.</p>

            <h2>Discrete Diffusion</h2>
            <p>Operates directly in the token space (e.g., a vocabulary of 50,000 words).</p>
            <ul>
                <li><span class="emphasis">Advantage:</span> It maintains perfect representational fidelity. The model's output is always a valid token from the vocabulary, eliminating the need for a final decoding step and avoiding errors associated with mapping from a continuous space back to discrete words.</li>
                <li><span class="emphasis">Disadvantage:</span> The discrete nature of text makes it difficult to define a smooth, gradual corruption process. The model learns to flip tokens, but it lacks the well-behaved mathematical properties of continuous spaces, which can make training less stable and limits the direct application of powerful acceleration techniques developed for continuous domains.</li>
            </ul>

            <h2>Continuous Diffusion</h2>
            <p>Operates in a high-dimensional vector space (e.g., 768-dimensional embeddings).</p>
            <ul>
                <li><span class="emphasis">Advantage:</span> This approach leverages a smooth, continuous latent space. It allows for the use of mathematically elegant tools like Gaussian noise for corruption and powerful Ordinary Differential Equation (ODE) solvers for a highly accelerated reverse process. As <span class="emphasis">Shabalin et al. (2025)</span> argue, this space can be made more suitable for diffusion by using context-rich <span class="emphasis">encodings</span> (from a pretrained model like BERT) instead of context-free token <span class="emphasis">embeddings</span>.</li>
                <li><span class="emphasis">Disadvantage:</span> It introduces a "domain gap." The model is not learning to generate text, but rather to generate <em>vectors</em> that represent text. This incurs significant computational overhead, as noted by Gong et al. (2023), and requires an additional, potentially error-prone, decoding step.</li>
            </ul>

            <hr class="section-divider heavy">

            <h1>Architecture Components</h1>

            <h2>1. The Forward Process (Corruption)</h2>
            
            <p>This is the foundational step that transforms a complex data distribution (natural language) into a simple one, creating a tractable learning problem for the model.</p>

            <h3>Discrete Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> The forward process defines a series of gradual corruption steps, creating the training targets for the model. By knowing exactly how tokens were replaced at each step, a neural network can be trained to reverse that specific corruption.</li>
                <li><span class="emphasis">Mechanism:</span> It operates as a fixed Markov chain, starting with a clean text sequence (x₀) and progressively replacing discrete tokens with a special [MASK] token until the sequence is fully degraded. This is often termed an "absorbing state" diffusion because once a token becomes [MASK], it stays masked.</li>
                <li><span class="emphasis">Recent Research:</span> Austin et al. (2021) introduced a generalized framework for discrete diffusion, proposing the use of structured transition matrices for corruption and highlighting the "absorbing state" as a critical design choice.</li>
            </ul>

            <h3>Continuous Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> Similar to the discrete approach, the goal is to create a tractable learning problem. However, instead of operating on discrete tokens, this process works in a continuous vector space.</li>
                <li><span class="emphasis">Mechanism:</span> This approach first maps tokens into a continuous vector space. While early models used simple token embeddings, a more advanced strategy is to use the final layer outputs of a pre-trained language model (e.g., BERT), referred to as encodings. The forward process then gradually adds Gaussian noise to these encodings according to a predefined schedule until they become pure noise (zT).</li>
                <li><span class="emphasis">Recent Research:</span> <span class="emphasis">Shabalin et al. (2025)</span> demonstrate that using contextual encodings is superior to using context-free embeddings, providing the diffusion model with a more suitable latent space for training.</li>
            </ul>

            <h2>2. The Reverse Process (Denoising)</h2>
            
            <p>This is the generative core of the model. It learns to reverse the corruption defined by the forward process, step-by-step, to generate new data.</p>

            <h3>Discrete Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To construct a coherent text sequence by starting from a fully masked sequence and iteratively filling in the [MASK] tokens. Each step in the reverse process fills in a few masks, gradually adding structure and meaning.</li>
                <li><span class="emphasis">Mechanism:</span> This process is a parameterized Markov chain that starts with a fully masked sequence (xT) and applies a learned transition kernel at each step to predict tokens for the masked positions.</li>
                <li><span class="emphasis">Recent Research:</span> <span class="emphasis">Zhang (2025)</span> provides theoretical justification for cosine scheduling during the reverse process, proving that it is Fisher-Rao optimal for masked discrete diffusion models.</li>
            </ul>

            <h3>Continuous Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To construct a coherent sequence of latent vectors by starting from pure Gaussian noise and iteratively denoising it. The final clean latent vectors are then decoded into text.</li>
                <li><span class="emphasis">Mechanism:</span> The process starts with a random Gaussian sample (zT) and uses a numerical solver to iteratively apply the denoising network over a fixed number of steps.</li>
                <li><span class="emphasis">Recent Research:</span> <span class="emphasis">Gong et al. (2023)</span> successfully adapted the <span class="emphasis">DPM-Solver++</span>, a fast ODE solver from the image domain, to their text diffusion model, achieving an <span class="emphasis">800x speedup</span> in sampling.</li>
            </ul>

            <h2>3. The Denoising Network</h2>
            
            <p>This is the parameterized component of the model that performs the learning task.</p>

            <h3>Discrete Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To predict the original, clean tokens from a partially masked sequence. At any given step t in the reverse process, this network takes the masked sequence xt as input and outputs a probability distribution over the vocabulary for the [MASK] positions.</li>
                <li><span class="emphasis">Mechanism:</span> There is a strong consensus on using a non-causal (encoder-only) Transformer architecture. Its bidirectional self-attention mechanism is crucial for filling in masked tokens, as context can appear on both the left and the right.</li>
            </ul>

            <h3>Continuous Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To predict the original, clean latent vectors (z₀) from a noisy version (zt). At any given step t, this network takes the noisy latents and the current timestep as input and outputs a prediction of the clean, noise-free latent vectors.</li>
                <li><span class="emphasis">Mechanism:</span> The model introduced by <span class="emphasis">Shabalin et al. (2025)</span> consists of a Diffusion Encoder (e.g., a frozen BERT), a Denoising Network (e.g., a 12-layer Transformer), and a Decoder (e.g., a trainable, smaller Transformer) to map the final, generated latent vectors back into text.</li>
            </ul>

            <h2>4. The Objective Function</h2>
            
            <p>This is the mathematical formulation of the training goal, defining the loss that the denoising network minimizes.</p>

            <h3>Discrete Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To train the denoising network to accurately predict the masked tokens. The theoretical goal is to maximize the log-likelihood of the data, which is intractable.</li>
                <li><span class="emphasis">Mechanism:</span> The model is trained by minimizing a tractable surrogate, the Variational Lower Bound (ELBO). A key insight, first formally derived by Austin et al. (2021), is that for an absorbing-state model, this complex objective simplifies into a weighted average of Masked Language Modeling (MLM) losses.</li>
            </ul>

            <h3>Continuous Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To train the denoising network to accurately predict the original, noise-free latent vectors.</li>
                <li><span class="emphasis">Mechanism:</span> As the model operates in a continuous space, the objective is typically a regression-style loss. The denoising network is trained by minimizing the mean-squared error (MSE) between the network's prediction of the clean latent vectors (ẑ₀) and the true clean latent vectors (z₀).</li>
            </ul>

            <h2>5. The Corruption Schedule</h2>
            
            <p>This component controls the rate or level of corruption applied during the forward process.</p>

            <h3>Discrete Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To define the probability of a token being masked at each step t of the forward process.</li>
                <li><span class="emphasis">Mechanism:</span> This is defined by a masking schedule. A simple and highly effective strategy has emerged: for each training sequence, sample a single masking ratio r uniformly from the interval [0, 1] and apply it.</li>
            </ul>

            <h3>Continuous Diffusion</h3>
            <ul>
                <li><span class="emphasis">Purpose:</span> To define how much Gaussian noise is added at each step t of the forward process. A well-designed schedule ensures the denoising task is manageable at every stage.</li>
                <li><span class="emphasis">Mechanism:</span> The schedule is a function, often denoted αt, that maps a time step t to a specific noise variance. Standard schedules from image diffusion (e.g., cosine) have been found to be suboptimal for text.</li>
            </ul>

            <hr class="section-divider heavy">

            <!-- References Section -->
            <section class="references">
                <h1>References</h1>
                
                <div class="reference-item">Austin, J., Johnson, D. D., Ho, J., Tarlow, D., & van den Berg, R. (2021). Structured denoising diffusion models in discrete state-spaces. In <em>Advances in Neural Information Processing Systems, 34</em>, 17981–17993.</div>
                
                <div class="reference-item">Chen, J., Zhang, A., Li, M., Smola, A., & Yang, D. (2023). A cheaper and better diffusion language model with soft-masked noise. <em>arXiv preprint arXiv:2304.04746</em>.</div>
                
                <div class="reference-item">Gladstone, A., Nanduru, G., Islam, M. M., Han, P., Ha, H., Chadha, A., Du, Y., Ji, H., Li, J., & Iqbal, T. (2025). Energy-Based Transformers are Scalable Learners and Thinkers. <em>arXiv preprint arXiv:2507.02092</em>.</div>
                
                <div class="reference-item">Gong, S., Li, M., Feng, J., Wu, Z., & Kong, L. (2023). DiffuSeq-v2: Bridging discrete and continuous text spaces for accelerated Seq2Seq diffusion models. In <em>Findings of the Association for Computational Linguistics: EMNLP 2023</em> (pp. 9868-9875).</div>
                
                <div class="reference-item">Li, Y., Zhou, K., Zhao, W. X., & Wen, J.-R. (2023). Diffusion models for non-autoregressive text generation: A survey. In <em>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)</em> (pp. 6692-6701).</div>
                
                <div class="reference-item">Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., & Li, C. (2025). Large language diffusion models. <em>arXiv preprint arXiv:2502.09992</em>.</div>
                
                <div class="reference-item">Ochs, S., & Habernal, I. (2025). Private synthetic text generation with diffusion models. In <em>Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.</div>
                
                <div class="reference-item">Prabhudesai, M., Wu, M., Zadeh, A., Fragkiadaki, K., & Pathak, D. (2025). Diffusion beats autoregressive in data-constrained settings. <em>arXiv preprint arXiv:2507.15857</em>.</div>
                
                <div class="reference-item">Sahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J. T., Rush, A., & Kuleshov, V. (2024). Simple and effective masked diffusion language models. In <em>Advances in Neural Information Processing Systems, 37</em>.</div>
                
                <div class="reference-item">Shabalin, A., Meshchaninov, V., Chimbulatov, E., Lapikov, V., Kim, R., Bartosh, G., Molchanov, D., Markov, S., & Vetrov, D. (2025). TEncDM: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings. In <em>The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)</em>.</div>
                
                <div class="reference-item">Tang, Z., Wang, P., Zhou, K., Li, J., Cao, Z., & Zhang, M. (2023). Can diffusion model achieve better performance in text generation? Bridging the gap between training and inference! In <em>Findings of the Association for Computational Linguistics: ACL 2023</em> (pp. 11359-11386).</div>
                
                <div class="reference-item">Ye, J., Gao, J., Gong, S., Zheng, L., Jiang, X., Li, Z., & Kong, L. (2025). Beyond autoregression: Discrete diffusion for complex reasoning and planning. In <em>International Conference on Learning Representations</em>.</div>
                
                <div class="reference-item">Yi, Q., Chen, X., Zhang, C., Zhou, Z., Zhu, L., & Kong, X. (2024). Diffusion models in text generation: A survey. <em>PeerJ Computer Science, 10</em>, e1905.</div>
                
                <div class="reference-item">Zhang, L. (2025). The cosine schedule is Fisher-Rao-optimal for masked discrete diffusion models. <em>arXiv preprint arXiv:2508.04884</em>.</div>
            </section>
        </main>

        <!-- Document Footer -->
        <footer class="document-footer">
            <div class="footer-grid">
                <div>
                    <strong>DOCUMENT ID:</strong><br>
                    TDM_DEV_GUIDE_v0.2
                </div>
                <div>
                    <strong>CLASSIFICATION:</strong><br>
                    TECHNICAL RESEARCH
                </div>
                <div>
                    <strong>AUTHOR:</strong><br>
                    JORGE A. ARROYO
                </div>
            </div>
            <hr style="border: 1px solid #ffffff; margin: 15px 0;">
            <div>SENS ADVISOR TECHNICAL DOCUMENTATION | © 2025 ALL RIGHTS RESERVED</div>
        </footer>
    </div>
</body>
</html>